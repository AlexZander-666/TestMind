# TestMind Environment Configuration
# Copy this file to .env and fill in your API keys

# ==============================================================================
# LLM Provider Configuration
# ==============================================================================

# Choose your LLM provider: openai, anthropic, gemini, ollama
LLM_PROVIDER=openai

# ==============================================================================
# OpenAI Configuration
# ==============================================================================

# OpenAI API Key (get from https://platform.openai.com/api-keys)
OPENAI_API_KEY=sk-your-openai-api-key-here

# Optional: Custom OpenAI API base URL
# OPENAI_API_BASE_URL=https://api.openai.com/v1

# OpenAI Model (gpt-4, gpt-4-turbo, gpt-3.5-turbo, etc.)
OPENAI_MODEL=gpt-4

# ==============================================================================
# Anthropic Configuration
# ==============================================================================

# Anthropic API Key (get from https://console.anthropic.com/)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# Anthropic Model (claude-3-opus, claude-3-sonnet, claude-3-haiku, etc.)
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# ==============================================================================
# Gemini Configuration (Google AI)
# ==============================================================================

# Gemini API Configuration
# For custom endpoints (e.g., proxy services), configure both URL and key
GEMINI_API_BASE_URL=https://metahk.zenymes.com/v1
GEMINI_API_KEY=sk-your-gemini-api-key-here

# Gemini Model (gemini-2.5-pro, gemini-pro, gemini-pro-vision, etc.)
GEMINI_MODEL=gemini-2.5-pro

# Alternative: Use OpenAI-compatible configuration for Gemini
# Set LLM_PROVIDER=openai and use these settings:
# OPENAI_API_BASE_URL=https://metahk.zenymes.com/v1
# OPENAI_API_KEY=sk-your-gemini-api-key-here
# OPENAI_MODEL=gemini-2.5-pro

# ==============================================================================
# Ollama Configuration (Local Models)
# ==============================================================================

# Ollama API Base URL (default: http://localhost:11434)
OLLAMA_API_BASE_URL=http://localhost:11434

# Ollama Model (codellama, llama2, mistral, etc.)
OLLAMA_MODEL=codellama

# ==============================================================================
# TestMind Configuration
# ==============================================================================

# Log Level: error, warn, info, debug
LOG_LEVEL=info

# Enable debug mode
DEBUG=false

# Test framework (jest, vitest, mocha)
TEST_FRAMEWORK=vitest

# ==============================================================================
# Observability Configuration
# ==============================================================================

# Sentry DSN for error tracking (optional)
# SENTRY_DSN=https://your-sentry-dsn-here

# Sentry Environment
# SENTRY_ENVIRONMENT=development

# Enable metrics collection
# ENABLE_METRICS=true

# ==============================================================================
# Development Configuration
# ==============================================================================

# Node environment
NODE_ENV=development

# Enable cache
ENABLE_CACHE=true

# Cache directory
CACHE_DIR=.testmind/cache

# ==============================================================================
# CI/CD Configuration
# ==============================================================================

# Non-interactive mode (for CI/CD)
# TESTMIND_NON_INTERACTIVE=true

# Auto-accept quality threshold (0-100)
# TESTMIND_AUTO_ACCEPT_THRESHOLD=80

# Batch processing mode
# TESTMIND_BATCH_MODE=true














