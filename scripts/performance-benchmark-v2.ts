/**
 * æ€§èƒ½åŸºå‡†æµ‹è¯• v2 - with LLMç¼“å­˜
 * 
 * å¯¹æ¯”æœ‰æ— ç¼“å­˜çš„æ€§èƒ½å·®å¼‚
 */

import { LLMService } from '../packages/core/src/llm/LLMService';
import { llmCache } from '../packages/core/src/llm/LLMCache';
import { TestGenerator } from '../packages/core/src/generation/TestGenerator';
import { ContextEngine } from '../packages/core/src/context/ContextEngine';
import * as path from 'path';

interface BenchmarkResult {
  scenario: string;
  totalTime: number;
  averageTime: number;
  cacheStats?: {
    hits: number;
    misses: number;
    hitRate: number;
  };
}

async function benchmarkWithoutCache() {
  console.log('\nğŸ“Š Scenario 1: Without Cache\n');
  console.log('â•'.repeat(60));

  const llmService = new LLMService();
  llmService.setCacheEnabled(false);

  const testPrompts = [
    'Generate unit test for add(a, b)',
    'Generate unit test for multiply(x, y)',
    'Generate unit test for divide(a, b)',
    'Generate unit test for subtract(x, y)',
    'Generate unit test for add(a, b)' // é‡å¤
  ];

  const startTime = Date.now();

  for (let i = 0; i < testPrompts.length; i++) {
    console.log(`  ç”Ÿæˆæµ‹è¯• ${i + 1}/${testPrompts.length}...`);
    
    const promptStart = Date.now();
    
    try {
      await llmService.generate({
        provider: 'openai',
        model: process.env.OPENAI_MODEL || 'gpt-4',
        prompt: testPrompts[i],
        temperature: 0.3,
        maxTokens: 1000
      });
      
      const duration = Date.now() - promptStart;
      console.log(`  âœ“ å®Œæˆ (${(duration / 1000).toFixed(1)}ç§’)`);
      
    } catch (error) {
      console.log(`  âŒ å¤±è´¥:`, error instanceof Error ? error.message : String(error));
    }
  }

  const totalTime = Date.now() - startTime;

  console.log(`\n  æ€»æ—¶é—´: ${(totalTime / 1000).toFixed(1)}ç§’`);
  console.log(`  å¹³å‡: ${(totalTime / testPrompts.length / 1000).toFixed(1)}ç§’/æµ‹è¯•\n`);

  return {
    scenario: 'Without Cache',
    totalTime,
    averageTime: totalTime / testPrompts.length
  };
}

async function benchmarkWithCache() {
  console.log('\nğŸ“Š Scenario 2: With Cache\n');
  console.log('â•'.repeat(60));

  const llmService = new LLMService();
  llmService.setCacheEnabled(true);
  llmCache.clear();
  llmCache.resetStats();

  const testPrompts = [
    'Generate unit test for add(a, b)',
    'Generate unit test for multiply(x, y)',
    'Generate unit test for divide(a, b)',
    'Generate unit test for subtract(x, y)',
    'Generate unit test for add(a, b)' // é‡å¤ - should hit cache
  ];

  const startTime = Date.now();

  for (let i = 0; i < testPrompts.length; i++) {
    console.log(`  ç”Ÿæˆæµ‹è¯• ${i + 1}/${testPrompts.length}...`);
    
    const promptStart = Date.now();
    
    try {
      const response = await llmService.generate({
        provider: 'openai',
        model: process.env.OPENAI_MODEL || 'gpt-4',
        prompt: testPrompts[i],
        temperature: 0.3,
        maxTokens: 1000
      });
      
      const duration = Date.now() - promptStart;
      const cached = response.finishReason === 'cached';
      
      console.log(`  âœ“ å®Œæˆ (${(duration / 1000).toFixed(1)}ç§’${cached ? ' - ç¼“å­˜å‘½ä¸­' : ''})`);
      
    } catch (error) {
      console.log(`  âŒ å¤±è´¥:`, error instanceof Error ? error.message : String(error));
    }
  }

  const totalTime = Date.now() - startTime;
  const cacheStats = llmCache.getStats();

  console.log(`\n  æ€»æ—¶é—´: ${(totalTime / 1000).toFixed(1)}ç§’`);
  console.log(`  å¹³å‡: ${(totalTime / testPrompts.length / 1000).toFixed(1)}ç§’/æµ‹è¯•`);
  console.log(`\n  ç¼“å­˜ç»Ÿè®¡:`);
  console.log(`    å‘½ä¸­: ${cacheStats.hits}`);
  console.log(`    æœªå‘½ä¸­: ${cacheStats.misses}`);
  console.log(`    å‘½ä¸­ç‡: ${(cacheStats.hitRate * 100).toFixed(1)}%`);
  console.log(`    èŠ‚çœè°ƒç”¨: ${cacheStats.totalSaved}æ¬¡\n`);

  return {
    scenario: 'With Cache',
    totalTime,
    averageTime: totalTime / testPrompts.length,
    cacheStats
  };
}

async function comparePerformance() {
  console.log('\nğŸš€ LLM Cache Performance Benchmark\n');
  console.log('â•'.repeat(60));
  console.log('\næµ‹è¯•5ä¸ªæç¤ºï¼ˆåŒ…å«1ä¸ªé‡å¤ï¼‰\n');

  try {
    const withoutCache = await benchmarkWithoutCache();
    const withCache = await benchmarkWithCache();

    console.log('\nğŸ“ˆ Performance Comparison\n');
    console.log('â•'.repeat(60));
    console.log(`\n  Without Cache: ${(withoutCache.totalTime / 1000).toFixed(1)}ç§’`);
    console.log(`  With Cache:    ${(withCache.totalTime / 1000).toFixed(1)}ç§’`);
    
    const speedup = withoutCache.totalTime / withCache.totalTime;
    console.log(`\n  âš¡ Speed up: ${speedup.toFixed(2)}x`);
    
    if (withCache.cacheStats) {
      const savings = (1 - withCache.totalTime / withoutCache.totalTime) * 100;
      console.log(`  ğŸ’° Time saved: ${savings.toFixed(1)}%`);
      console.log(`  ğŸ¯ Cache hit rate: ${(withCache.cacheStats.hitRate * 100).toFixed(1)}%`);
    }

    console.log('\nâ•'.repeat(60));
    console.log('\nâœ… Benchmark Complete!\n');

    // ç”ŸæˆæŠ¥å‘Š
    const report = generateReport([withoutCache, withCache]);
    await fs.writeFile('performance-benchmark-report.md', report, 'utf-8');
    console.log('ğŸ“„ Report saved: performance-benchmark-report.md\n');

  } catch (error) {
    console.error('\nâŒ Benchmark failed:', error);
    process.exit(1);
  }
}

function generateReport(results: BenchmarkResult[]): string {
  return `# LLM Cache Performance Benchmark Report

**Date**: ${new Date().toISOString().split('T')[0]}

## Summary

${results.map(r => `### ${r.scenario}

- **Total Time**: ${(r.totalTime / 1000).toFixed(1)}s
- **Average Time**: ${(r.averageTime / 1000).toFixed(1)}s per test
${r.cacheStats ? `
**Cache Statistics**:
- Hits: ${r.cacheStats.hits}
- Misses: ${r.cacheStats.misses}
- Hit Rate: ${(r.cacheStats.hitRate * 100).toFixed(1)}%
- API Calls Saved: ${r.cacheStats.hits}
` : ''}
`).join('\n')}

## Performance Improvement

${results.length === 2 ? `
- **Speed Up**: ${(results[0].totalTime / results[1].totalTime).toFixed(2)}x
- **Time Saved**: ${((1 - results[1].totalTime / results[0].totalTime) * 100).toFixed(1)}%
${results[1].cacheStats ? `- **Cache Hit Rate**: ${(results[1].cacheStats.hitRate * 100).toFixed(1)}%` : ''}

## Conclusion

LLM caching provides significant performance improvement:
- Cached responses return in <1s vs ${(results[0].averageTime / 1000).toFixed(1)}s for API calls
- Cost savings: ${results[1].cacheStats?.hits || 0} API calls avoided
- Recommended for production use
` : ''}

---

*Generated by performance-benchmark-v2.ts*
`;
}

// è¿è¡ŒåŸºå‡†æµ‹è¯•
if (require.main === module) {
  comparePerformance().catch(console.error);
}

 * æ€§èƒ½åŸºå‡†æµ‹è¯• v2 - with LLMç¼“å­˜